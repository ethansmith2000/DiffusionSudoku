{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sudoku_mrv import generate_board, verify_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = generate_board(completeness=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 1, 6, 3, 7, 4, 2, 8, 9],\n",
       " [3, 4, 7, 9, 8, 2, 5, 6, 1],\n",
       " [2, 8, 9, 6, 1, 5, 4, 3, 7],\n",
       " [8, 9, 2, 7, 5, 3, 6, 1, 4],\n",
       " [4, 3, 1, 8, 2, 6, 9, 7, 5],\n",
       " [7, 6, 5, 1, 4, 9, 3, 2, 8],\n",
       " [9, 7, 3, 5, 6, 1, 8, 4, 2],\n",
       " [1, 5, 4, 2, 3, 8, 7, 9, 6],\n",
       " [6, 2, 8, 4, 9, 7, 1, 5, 3]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_board(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2), nn.GELU(), nn.Linear(dim * mult * 2, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x) + x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3 * heads, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=self.heads), self.to_qkv(x).chunk(3, dim=-1))\n",
    "        attn_out = torch.nn.functional.scaled_dot_product_attention(q, k, v)\n",
    "        out = rearrange(attn_out, \"b h n d -> b n (h d)\", h=self.heads)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(dim, heads)\n",
    "        self.ff = FeedForward(dim)\n",
    "        self.attn_norm = nn.LayerNorm(dim)\n",
    "        self.ff_norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.attn_norm(x))\n",
    "        x = x + self.ff(self.ff_norm(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, head_dim=64, heads=8, num_classes=10, depth=12, ff_mult=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_classes, head_dim)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(head_dim, heads) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(head_dim)\n",
    "        self.to_logits = nn.Linear(head_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        return self.to_logits(x)\n",
    "\n",
    "\n",
    "class DiscreteDiffusion(nn.Module):\n",
    "\n",
    "    def __init__(self, model, num_classes=10, head_dim=64, heads=8, depth=12, ff_mult=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.model = Transformer(head_dim, heads, num_classes, depth, ff_mult, dropout)\n",
    "\n",
    "    def forward(self, board_bhw, labels=None):\n",
    "        \"\"\"\n",
    "        forward and compute loss\n",
    "        \"\"\"\n",
    "        b, h, w = board_bhw.shape\n",
    "        board_bl = board_bhw.flatten(1)\n",
    "        preds_bl = self.model(board_bl)\n",
    "        if labels is not None:\n",
    "            loss = nn.functional.cross_entropy(preds_bl, labels.flatten(1))\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        return preds_bl, loss\n",
    "\n",
    "    def gen_sample(self, shape=(1, 9, 9)):\n",
    "        board = torch.zeros(shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
